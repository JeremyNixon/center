

@extends('/blog/blog-base')
@section('body')

<html><head><title>p1_writeup.pdf</title><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol{margin:0;padding:0}.c4{color:#000000;font-weight:normal;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Arial";font-style:normal}.c3{color:#000000;font-weight:normal;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c7{color:#000000;font-weight:normal;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Arial";font-style:normal}.c12{color:#000000;font-weight:normal;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Courier New";font-style:normal}.c8{padding-top:0pt;padding-bottom:5pt;line-height:1.15;text-align:center;direction:ltr}.c1{padding-top:0pt;padding-bottom:5pt;line-height:1.15;text-align:justify;direction:ltr}.c2{padding-top:0pt;padding-bottom:5pt;line-height:1.15;text-align:left;direction:ltr}.c0{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left;direction:ltr}.c10{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c6{text-indent:54.2pt}.c9{text-indent:21.4pt}.c5{text-indent:17pt}.c11{text-indent:456.2pt}.c13{text-indent:48.7pt}.title{padding-top:24pt;color:#000000;font-weight:bold;font-size:36pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:18pt;color:#666666;font-size:24pt;padding-bottom:4pt;font-family:"Georgia";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:24pt;color:#000000;font-weight:bold;font-size:24pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-weight:bold;font-size:18pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:14pt;color:#000000;font-weight:bold;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:12pt;color:#000000;font-weight:bold;font-size:12pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:11pt;color:#000000;font-weight:bold;font-size:11pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:10pt;color:#000000;font-weight:bold;font-size:10pt;padding-bottom:2pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}</style></head><body class="c10"><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 621.33px; height: 803.91px;"><img alt="" src="images/image04.png" style="width: 621.33px; height: 803.91px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><hr></p><p class="c2"><span class="c3">Computer Science 181 (Spring 2015)</span></p><p class="c2"><span class="c7">Practical 1: Predicting the Efficiency of Organic Photovoltaics</span></p><p class="c8"><span class="c3">Team Name: 999 DRAGON ARMY999 Team Members: Brandon Sim, Jeremy Nixon, Lydia Chen</span></p><p class="c2"><span class="c7">Introduction</span></p><p class="c1"><span class="c3">This paper describes our approach to predicting HOMO-LUMO gaps for photovoltaic molecules using a machine-learning based approach, rather than traditional quantum chemical calculations, which may be prohibitively expensive to run on a large set of molecules. We consider a variety of regression techniques, and dive especially deeply into the random forest regressor. We also generate a variety of additional features to improve the quality of our predictions, and report our results for various feature sets and classifiers. We obtain a final RMSE of around 6.6%.</span></p><p class="c2"><span class="c7">Linear Regressors</span></p><p class="c1"><span class="c3">We began our approach with a survey of various linear regressors, including various regularization techniques (L1, L2, L1 and L2). We trained the default feature set using a ridge regression, a LASSO regression, and a combination of these regularization techniques, the elastic net. We performed 10-fold cross validation on a variety of &alpha; values for the ridge regression as well as the LASSO regression. The results are summarized in the results section. However, because of limited computational time, we decided to focus on one type of regressor, the random forest regressor, and did not test linear regressors with any regularization type in depth. In the future, we would like to consider elastic nets in particular due to their ability to generate sparse solutions (without the drawbacks of pure LASSO) on the final feature set that we generated.</span></p><p class="c2"><span class="c7">Random Forest Regressor</span></p><p class="c1"><span class="c3">Given the nature of the input data, a regression tree is an attractive method for this particular ma- chine learning problem. Regression trees are invariant to transformations of the input data, which in this case is largely binary with some exceptions in the features we selected later. At each node, the tree considers a random sample K of the features. The node chooses, out of the K features under consideration, the feature that minimizes the sum of squared errors in splitting the data into two more nodes. This process is repeated at each node until each node contains one data point. The decision tree is resilient to features with poor predictive power because even if all K features considered at a particular node are useless in predicting the target value, the tree will simply make a split as described above and the resulting subtrees will (likely) be split on critical predictors as the of features, tree grows rather deeper than [5]. selecting This allows for or us excluding to focus on individual generating features. and testing For most groups purposes of different K =</span></p><p class="c2"><span class="c3">types</span></p><p class="c2"><span class="c12">/</span></p><p class="c1 c11"><span class="c3">M where M is the number of features is a computationally efficient and sufficiently large number for N [5]. We performed a cross-validation test and actually found that considering a random sample</span></p><p class="c2"><span class="c3">1</span></p><hr style="page-break-before:always;display:none;"><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 621.33px; height: 803.91px;"><img alt="" src="images/image06.png" style="width: 621.33px; height: 803.91px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><hr></p><p class="c2"><span class="c3">Computer Science 181 (Spring 2015)</span></p><p class="c2"><span class="c3">of</span></p><p class="c2"><span class="c12">/</span></p><p class="c2 c9"><span class="c3">M features slightly outperformed the default of considering all M features at each node, both in runtime and accuracy.</span></p><p class="c1"><span class="c3">However, a single tree is rarely accurate, since there is the chance that a tree just happens to consider poor predictors at every node, or that certain predictors overfit particular subsets of the data. To address the problem of overfitting we used a random forest, which trains N trees on samples of the training data and then averages the predictions from each of the N trees, which decreases the bias that often occurs in one tree. Part of the improvement that comes from using random forests instead of single regression trees is that each tree is trained on random bootstrapped samples of the training data, allowing for randomness that will be averaged out over the complete forest. Our final prediction &circ;y can be expressed as the average of the predictions of N regression trees &circ;y</span></p><p class="c2"><span class="c4">n</span></p><p class="c2"><span class="c3">.</span></p><p class="c2"><span class="c3">&circ;y =</span></p><p class="c8"><span class="c3">N 1</span></p><p class="c2"><span class="c4">N&sum;</span></p><p class="c2"><span class="c3">&circ;y</span></p><p class="c8"><span class="c3">n n=0 As expected, the number of trees N is a particularly important parameter to tune for random forests. Two few trees can result in high variance and less accuracy on predictions. Too many trees can significantly slow down computing. On average, the runtime of a random forest is quasilinear in the number of trees, O(NM logN) [3]. In practice, however, running the forests on larger features sets showed a more than linear increase in runtime. We tuned this parameter by cross-validating random forests of varying sizes on the original feature set. The results can be seen in Figure 1.</span></p><p class="c2"><span class="c3">Figure 1: Mean absolute error for 256 features with different sizes of random forest</span></p><p class="c2"><span class="c3">2</span></p><hr style="page-break-before:always;display:none;"><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 621.33px; height: 803.91px;"><img alt="" src="images/image05.png" style="width: 621.33px; height: 803.91px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><hr></p><p class="c2"><span class="c3">Computer Science 181 (Spring 2015)</span></p><p class="c1 c5"><span class="c3">Ignoring the slight noise due to randomness, the marginal improvement in predictive power from adding trees to the random forest flattens out around 30 trees, although mean absolute error continues to decrease as the number of trees increases.</span></p><p class="c2"><span class="c7">Feature Selection</span></p><p class="c2"><span class="c3">Using rdkit, we generated several new sets of features, which are summarized in Table 1.</span></p><p class="c2"><span class="c3">Feature Type Description Parameters Morgan/Circular Fingerprint</span></p><p class="c2"><span class="c3">Uses the Morgan algorithm to generate a bit vector for each atom and its surrounding neighborhood (defined by parameter radius)</span></p><p class="c2"><span class="c3">Radius, Number of Bits, Features Invariant</span></p><p class="c2"><span class="c3">Topological Fingerprint Identifies and hashes bond paths to generate a</span></p><p class="c2"><span class="c3">bit vector</span></p><p class="c2"><span class="c3">Number of Bits</span></p><p class="c2"><span class="c3">MACCS Keys 166 pre-identified substructures used to generate</span></p><p class="c2"><span class="c3">a fingerprint</span></p><p class="c2"><span class="c3">None</span></p><p class="c2"><span class="c3">Chemical Properties Various binary and nonbinary chemical descrip-</span></p><p class="c2"><span class="c3">tors for molecules</span></p><p class="c2"><span class="c3">Features</span></p><p class="c2"><span class="c3">Table 1: Summary of various fingerprints and features generated</span></p><p class="c2"><span class="c3">General Strategy</span></p><p class="c1"><span class="c3">Our general strategy with feature selection was to generate a large variety of features, then decide quantitatively through cross-validation which features would be most useful for inclusion in our final model. Below we discuss several classes of features that we generated and considered; our final submission, as discussed more in depth in our results section, uses a combination of the Morgan (with and without features) fingerprints and the chemical descriptor features.</span></p><p class="c2"><span class="c3">Morgan/Circular Fingerprint</span></p><p class="c2"><span class="c3">The Morgan algorithm generates a bit vector in a given radius around each atom of a molecule. The generated vectors are then hashed to a vector of a user-specified length.</span></p><p class="c1"><span class="c3">Radius: The radius parameter defines the size of the atom environment that the algorithm con- siders when considering each individual atom. A radius of 1, for example, considers the atom and atoms one chemical bond away. Literature suggests that a radius of 2-3 is standard, and through our own cross-validation tests we found that radii above 5 did not significantly improve predictive power.</span></p><p class="c2"><span class="c3">Number of bits: Since the Morgan algorithm hashes the results of the analysis of each atom to a bit vector of a specified length, the probability of collision will decrease with a larger number</span></p><p class="c2"><span class="c3">3</span></p><hr style="page-break-before:always;display:none;"><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 621.33px; height: 803.91px;"><img alt="" src="images/image08.png" style="width: 621.33px; height: 803.91px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><hr></p><p class="c2"><span class="c3">Computer Science 181 (Spring 2015)</span></p><p class="c1"><span class="c3">of bits, meaning that we will get a more descriptive fingerprint. We tried vectors of length 256 (as given), 512, and 1024, with cross-validation tests confirming that more bits resulted in better predictions (albeit with decreasing marginal returns). The limiting factor on tuning this parameter was computing power.</span></p><p class="c1"><span class="c3">Feature-invariant versus atom-invariant: A parameter that ended up being key to our predictive power was toggling the feature-invariant parameter in rdkit. The first runs we did had the feature- invariant turned on, meaning that the Morgan algorithm looked for pre-defined features based on chemical standards that have proven, in general, to be useful at differentiating molecules. On the other hand, the atom-invariant approach looks for different connectivity information for each atom being considered&ndash;number of rings, number of attached hydrogens, etc.</span></p><p class="c1"><span class="c3">As noted in the rdkit documentation [2], these two approaches (feature-invariant, atom-invariant) can sometimes lead to very different similarity scores between the same molecules, and we found that in this case the features that were generated were at least partially orthogonal; in cross- validation tests we saw a marked improvement by concatenating both sets of features and using them as our predictors as opposed to either set alone. The results suggest that the HOMO-LUMO gap is best predicted by some combination of feature and atom invariant features.</span></p><p class="c2"><span class="c3">Topological Fingerprint</span></p><p class="c1"><span class="c3">Topological fingerprints identify and hash topological paths to bit vectors. Therefore, as with Morgan Fingerprints, longer bit vectors decrease the probability of collision and allow for more useful fingerprints. The topological features that were identified did not end up outperforming the Morgan features and also did not seem to add any additional predictive power when concatenated with the Morgan feature sets.</span></p><p class="c2"><span class="c3">MACCS Keys</span></p><p class="c1"><span class="c3">Much like the feature-invariant approach in the Morgan Fingerprint, MACCS Keys uses a set of 166 pre-identified substructures as features [1]. As it turns out, the rdkit documentation notes that the MACCS Keys and the Morgan feature invariants are based on similar sets of substructures, and so it is not surprising that MACCS failed to outperform even when concatenated with other feature sets.</span></p><p class="c2"><span class="c3">Chemical Properties</span></p><p class="c1"><span class="c3">We considered a variety of chemical descriptors, which includes properties calculated about a molecule based on its molecular structure. Some of these descriptors may have been partially or completely encoded in hashed form via some of the other fingerprint-based methods that we tried, and this correlation may have led to some bias; however, overall, we found that the addition of these chemical descriptor features yielded improvements to our model. The full list of chemical descriptors we considered is listed in this RDKit documentation link. From this list, we extracted</span></p><p class="c2"><span class="c3">4</span></p><hr style="page-break-before:always;display:none;"><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 621.33px; height: 803.91px;"><img alt="" src="images/image07.png" style="width: 621.33px; height: 803.91px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><hr></p><p class="c2"><span class="c3">Computer Science 181 (Spring 2015)</span></p><p class="c1"><span class="c3">183 features, some of which include molecular weight, number of rings, and various other surface area-related features, such as approximate van der Waals surface area and topological polar surface area. A full list of the features we used can be found in the appendix. The types of data in this feature set, as seen in the appendix, took on a range of values, ranging from binary to integer (such as number of rings, etc.) to continuous variables (such as molecular mass or VSA).</span></p><p class="c1"><span class="c3">Because the rest of our data was binary, we performed LASSO regressions in an attempt to perform feature selection, although we were somewhat confident that our random forest regressor would not overfit or run into huge problems with a mix of continuous and binary data. We also performed this analysis to attempt to see if the most important features as chosen by regularization were reasonable, as these chemical descriptors were more easily mapped to quantities that could be intu- itively understood to have some effect on the HOMO-LUMO gap (unlike the fingerprinting results, where the hash made it difficult to tell which feature was impacting which bit). We performed regularization at various &alpha; levels, using LASSO due to its shrinkage properties, and found various subsets of the features which seemed to be most important. Specifically, we found a certain subset of features to be useful at the &alpha; = 0.01 level, which is found in the appendix here. We see that some of the most indicative features are the various van der Waals surface area calculations, which make sense intuitively as the size of the surface area in a van der Waals sense help govern the bonding properties and thus the HOMO-LUMO gap. However, as we are not chemists, we simply accept this preliminary eyeball test; in our final run, however, we ended up using all 183 features, as the random forest regressor seemed to be adequately not overfitting the data, even with extraneous features (seen in the table below). We include this discussion of features anyway because the selec- tion of these features as important may lead to some chemical understanding of important features to look at, or may inspire further generation of other similar features which may prove useful even in machine learning approaches.</span></p><p class="c2"><span class="c3">RMSE (100k training) (Mean over 5 runs each)</span></p><p class="c2"><span class="c3">Number of Trees</span></p><p class="c2"><span class="c3">Chem Features 30 40 50 60 None n/a 0.120441 0.119135 n/a &alpha; = 0 (all) 0.116613 0.115653 0.114607 0.114128 &alpha; = 0.001 0.116864 n/a n/a n/a &alpha; = 0.01 0.117339 n/a n/a n/a &alpha; = 0.1 0.117965 n/a n/an n/a</span></p><p class="c2"><span class="c3">Table 2: RMSE results for various regularization parameters on chemical descriptor feature set</span></p><p class="c2"><span class="c7">Results</span></p><p class="c1"><span class="c3">Below we present a comprehensive table which document some (but not all) of the training and validation that we considered when deciding which paths to pursue during this practical. Of key importance was the lack of computing power of our team; processing took place on two laptop</span></p><p class="c2"><span class="c3">5</span></p><hr style="page-break-before:always;display:none;"><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 621.33px; height: 803.91px;"><img alt="" src="images/image01.png" style="width: 621.33px; height: 803.91px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><hr></p><p class="c2"><span class="c3">Computer Science 181 (Spring 2015)</span></p><p class="c1"><span class="c3">machines throughout the practical (only two of the laptops owned had sufficient RAM to load the data necessary). Given this constraint, many entries in the testing matrix below are blank due to time constraints. Because of theoretical reasons discussed above, we decided to hone in on one regression model, the random forest regressor, and focus on feature engineering and selection.</span></p><p class="c1"><span class="c3">Here, we present RMSE results in tabular form: across we have various feature sets, and verti- cally we used various types of regressors. The feature set letters correspond to the following list (separated for readability):</span></p><p class="c2"><span class="c3">A: Morgan fingerprints, radius 1, 256-bit, with features</span></p><p class="c2"><span class="c3">B: Topological fingerprinting, 1024 bits</span></p><p class="c2"><span class="c3">C: Morgan fingerprints, radius 2, 512-bit, with features</span></p><p class="c2"><span class="c3">D: Morgan fingerprints, radius 3, 512-bit, with features</span></p><p class="c2"><span class="c3">E: Morgan fingerprints, radius 3, 1024-bit, with features</span></p><p class="c2"><span class="c3">F: Morgan fingerprints, radius 3, 512-bit, no features</span></p><p class="c2"><span class="c3">G: Combination: Morgan fingerprints, radius 3, 512-bit, features and no features</span></p><p class="c2"><span class="c3">H: Combination: Morgan fingerprints, radius 3, 512-bit, features and no features; and chemical</span></p><p class="c2"><span class="c3">features</span></p><p class="c2 c13"><span class="c4">Results Feature Set (across) Method (down) A B C D E F G H Mean 0.40682 OLS Regression 0.29845 SGD Regression (out of the box) 0.29857 SGD Elastic Net (L1+L2) 0.29863 Support Vector Regression 0.29728 Random Forest Regression (K = n</span></p><p class="c2"><span class="c4">features</span></p><p class="c2 c6"><span class="c4">), 30 trees</span></p><p class="c2"><span class="c4">0.27213</span></p><p class="c2"><span class="c4">Random (K = /</span></p><p class="c2"><span class="c4">n</span></p><p class="c2"><span class="c4">Forest features</span></p><p class="c2"><span class="c4">),</span></p><p class="c2"><span class="c4">Regression</span></p><p class="c2"><span class="c4">30 trees</span></p><p class="c2"><span class="c4">0.27205 0.13009 0.11343 0.10152 0.09896 0.08201 0.07056 0.06765</span></p><p class="c2"><span class="c4">Random (K = /</span></p><p class="c2"><span class="c4">n</span></p><p class="c2"><span class="c4">Forest features</span></p><p class="c2"><span class="c4">),</span></p><p class="c2"><span class="c4">Regression</span></p><p class="c2"><span class="c4">60 trees</span></p><p class="c2"><span class="c4">0.06566*</span></p><p class="c2"><span class="c3">Table 3: Matrix of RMSE results for various feature sets and regressors; (*) is the final submission</span></p><p class="c2 c5"><span class="c3">As can be seen in the above table, we tried various regressors, but decided to focus on the random forest regressor and tuning parameters for that regressor due to limited computational</span></p><p class="c2"><span class="c3">6</span></p><hr style="page-break-before:always;display:none;"><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 621.33px; height: 803.91px;"><img alt="" src="images/image00.png" style="width: 621.33px; height: 803.91px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><hr></p><p class="c2"><span class="c3">Computer Science 181 (Spring 2015)</span></p><p class="c1"><span class="c3">power and due to the attractive features of the random forest as discussed in the body of the paper. Through feature selection improvements, we make gains and eventually come to an RMSE of around 6.6%. This result held up in the test set, lending credence to our belief about random forests being resilient to overfitting.</span></p><p class="c2"><span class="c7">References</span></p><p class="c2"><span class="c3">[1] Dalke, Andrew. MACCS key 44. Scientific AB, 2013.</span></p><p class="c2"><span class="c3">[2] Landrum, Greg. The RDKit Documentation. 2014.</span></p><p class="c2"><span class="c3">[3] Loupe, Gilles. Understanding Random Forest: From Theory to Practice. University of Liege,</span></p><p class="c2"><span class="c3">2014.</span></p><p class="c2"><span class="c3">[4] Pedregosa, Fabian, et al. Scikit-learn: Machine Learning in Python. JMLR 12 pp. 2825-2830,</span></p><p class="c2"><span class="c3">2011.</span></p><p class="c2"><span class="c3">[5] Steinberg, Dan, Mikhail Golovnya, and N. Scott Cardell. A Brief Overview to Random Forests.</span></p><p class="c2"><span class="c3">Salford Systems, 2004.</span></p><p class="c2"><span class="c3">7</span></p><hr style="page-break-before:always;display:none;"><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 621.33px; height: 803.91px;"><img alt="" src="images/image03.png" style="width: 621.33px; height: 803.91px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><hr></p><p class="c2"><span class="c3">Computer Science 181 (Spring 2015)</span></p><p class="c2"><span class="c7">Appendix</span></p><p class="c2"><span class="c4">BalabanJ NumAromaticHeterocycles SlogP VSA5 fr alkyl halide fr phenol noOrthoHbond BertzCT NumAromaticRings SlogP VSA6 fr allylic oxid fr phos acid Chi0 NumHAcceptors SlogP VSA7 fr amide fr phos ester Chi0n NumHDonors SlogP VSA8 fr amidine fr piperdine Chi0v NumHeteroatoms SlogP VSA9 fr aniline fr piperzine Chi1 NumRotatableBonds TPSA fr aryl methyl fr priamide Chi1n NumSaturatedCarbocycles VSA EState1 fr azide fr prisulfonamd Chi1v NumSaturatedHeterocycles VSA EState10 fr azo fr pyridine Chi2n NumSaturatedRings VSA EState2 fr barbitur fr quatN Chi2v PEOE VSA1 VSA EState3 fr benzene fr sulfide Chi3n PEOE VSA10 VSA EState4 fr benzodiazepine fr sulfonamd Chi3v PEOE VSA11 VSA EState5 fr bicyclic fr sulfone Chi4n PEOE VSA12 VSA EState6 fr diazo fr term acetylene Chi4v PEOE VSA13 VSA EState7 fr dihydropyridine fr tetrazole EState VSA1 PEOE VSA14 VSA EState8 fr epoxide fr thiazole EState VSA10 PEOE VSA2 VSA EState9 fr ester fr thiocyan EState VSA11 PEOE VSA3 fr Al COO fr ether fr thiophene EState VSA2 PEOE VSA4 fr Al OH fr furan fr unbrch alkane EState VSA3 PEOE VSA5 fr Al OH noTert fr guanido fr urea EState VSA4 PEOE VSA6 fr ArN fr halogen EState VSA5 PEOE VSA7 fr Ar COO fr hdrzine EState VSA6 PEOE VSA8 fr Ar N fr hdrzone EState VSA7 PEOE VSA9 fr Ar NH fr imidazole EState VSA8 RingCount fr Ar OH fr imide EState VSA9 SMR VSA1 fr COO fr isocyan FractionCSP3 SMR VSA10 fr COO2 fr isothiocyan HallKierAlpha SMR VSA2 fr C O fr ketone HeavyAtomCount SMR VSA3 fr C O noCOO fr ketone Topliss Ipc SMR VSA4 fr C S fr lactam Kappa1 SMR VSA5 fr HOCCN fr lactone Kappa2 SMR VSA6 fr Imine fr methoxy Kappa3 SMR VSA7 fr NH0 fr morpholine LabuteASA SMR VSA8 fr NH1 fr nitrile MolLogP SMR VSA9 fr NH2 fr nitro MolMR SlogP VSA1 fr N O fr nitro arom NHOHCount SlogP VSA10 fr Ndealkylation1 fr nitro arom nonortho NOCount SlogP VSA11 fr Ndealkylation2 fr nitroso NumAliphaticCarbocycles SlogP VSA12 fr Nhpyrrole fr oxazole NumAliphaticHeterocycles SlogP VSA2 fr SH fr oxime NumAliphaticRings SlogP VSA3 fr aldehyde fr para hydroxylation NumAromaticCarbocycles SlogP VSA4 fr alkyl carbamate fr phenol</span></p><p class="c2"><span class="c3">Table 4: List of chemical features used</span></p><p class="c2"><span class="c3">8</span></p><hr style="page-break-before:always;display:none;"><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 621.33px; height: 803.91px;"><img alt="" src="images/image02.png" style="width: 621.33px; height: 803.91px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><hr></p><p class="c2"><span class="c3">Computer Science 181 (Spring 2015)</span></p><p class="c2"><span class="c3">BertzCT PEOE VSA14 SlogP VSA1 EState VSA2 PEOE VSA3 SlogP VSA12 EState VSA3 PEOE VSA4 SlogP VSA2 EState VSA4 PEOE VSA5 SlogP VSA3 EState VSA5 PEOE VSA7 SlogP VSA5 EState VSA6 PEOE VSA8 SlogP VSA6 EState VSA8 PEOE VSA9 TPSA EState VSA9 SMR VSA10 VSA EState9 MolMR SMR VSA3 fr allylic oxid PEOE VSA1 SMR VSA7 fr bicyclic PEOE VSA11 SMR VSA9 fr pyridine</span></p><p class="c2"><span class="c3">Table 5: Features selected by LASSO at &alpha; = 0.01 level.</span></p><p class="c2"><span class="c3">9</span></p></body></html>

@stop